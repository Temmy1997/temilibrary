# Amazon Simple Storage Service(S3)
* S3 is a cloud-based object storage service.
* Think of it as a place to store files (objects) in the cloud.
* It's highly scalable, durable, and secure.
* S3 is a key value store
Key -Name of the file(object)
Value - The content of the file(object)
* S3 has an indefinate storage (It expands storage according to your need)

## Amazon S3 Buckets 
* S3 allows to store objects(files) in buckets(directories).
* Buckets are like folders or containers for your objects.
* All bucket must has global unique name (Nobody can use the same bukect name twice)
* Buckets are defined at the region level
* When creating a buckets:
    * Buckets name cannot be empty
    * NO uppercase and underscore
    * Must only have lowercase or numbers
    * Min of 3 characters
    * Max of 63 charachers
    * Not looks like an IP address  
* Even though S3 is a global view, we have to specify a specific region when creatiing a buckets 

## Amazon S3 -- Objects 
* Objects are the files you store in S3.
* Objects (files) have a key.
* The key is the full path of the file.
    - s3://my-bucket/my_file.txt
* All objects has a max size of 5TB(5000GB) storage space.
* If object size more than 5TB, AWS will use "multi-part upload" to upload the file into several parts. 

## Amazon S3 use cases 
1. Backup and storage
2. Disaster recovery 
3. Archive 
4. Hybrid cloud storage 
5. Application hosting 
6. media hosting 
7. Static website 
8. Software deliver 


## METADATA
* Metadata is a set of name-value pairs.Types are: 
    * System-Defined Objets Metadata: Automatic Created by AWS 
    * User-Defined Objects Metadata: Name always start with [x-amz-meta]
Note: Every resources created ia own by the root user 

## Aamzon S3 Security 
1. User-Based 
  * IAM POLICIES: It define what actions a user or role can perform on which resources.

2. Resource BASED 
   *  BUCKET POLICIES: Attaching a bucket policy to an S3 bucket, defines who can access that bucket and what actions they can perform.
   *  Object Access Control Lists (ACL)  (CAN BE DIABLED) 
   *  Bucket Access Control Lists

## NOTES: In which situation can an IAM pricipal acess an s3 object: 
*  The IAM permission/resource policy allows it 

3. Encryption: Encrypting objects in Amazon S3 using encryption keys 

## S3 Bucket policies
1. JSON Based polies 
   * Resources Bloack: Buckets and object the policy applies to. 
   * Effect: Whether the policy allows (Allow) or denies (Deny) access.
   * Pricipal Block : The user, role, or service that is granted or denied the access. 
   * Acion Block: The specific operation that the principal is allowed or denied to perform (e.g., s3:GetObject, s3:PutObject).

FOR EXAMPLE: 
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicRead",
            "Effect": "Allow",
            "Principal": "*",
            "Action": [
                "s3:GetObject",
                "s3:GetObjectVersion"
            ],
            "Resource": "arn:aws:s3:::temi1997/*"
        }
    ]
}

```

2. S3 Bucket policy 
   * Give piblic access to the bucket 
   * Force object to be encrypted at upload 
   * Cross account (grant access to another account)

# Amazon S3 Versioning 
* Versioning in Amazon S3 is a feature that allows you to keep multiple versions of an object in the same bucket. 

### What Versioning Does:
1. Keeps History:
    - When you enable versioning on an S3 bucket, every time you upload, modify, or delete an object, S3 creates a new version of that object.
    - This means you can go back and retrieve previous versions of your files.

2. Protects Against Accidental Deletions
    - If you accidentally delete an object, it's not permanently gone. S3 creates a delete marker, but the previous versions of the object remain.
    - You can easily restore the deleted object by removing the delete marker.
3. Protects Against Overwrites:
    - If you accidentally overwrite an object with incorrect data, you can retrieve the previous version.

* Versioning is done at the bucket level.

## Key Concepts:

1. Version ID:
- Each version of an object has a unique version ID.
- This ID allows you to retrieve specific versions of an object.

2. Delete Marker:
- When you delete an object in a versioned bucket, S3 creates a delete marker.
- The object is not actually deleted; it's just marked as deleted.
- To permanently delete an object, you must delete all of its versions, including the delete marker.

3. Suspended vs. Enabled:
- Versioning can be in one of two states: Enabled or Suspended.
- When enabled, versioning is active.
- When suspended, new versions are no longer created, but existing versions remain.


NOTE: any file that is not versioned prior to enabling versioning will have version 'null'

## Amazon S3 Replication (CRR & SRR)
* Amazon S3 Replication is a feature that automatically copies objects from one S3 bucket to another. 
* This enables you to create backups, reduce latency, and meet compliance requirements. 
* Replication only works if versining is enbaled 

CRR (CROSS-REGION REPLICATION): 
* Replicates objects between S3 buckets in different AWS Regions.
* Useful for disaster recovery and compliance.

SRR: SAME-REGION REPLICATION
* Replicates objects between S3 buckets in the same AWS Region.
* Useful for log aggregation and separating production and test data. 

NOTE: 

YOU MUST GIVE IAM PERMISSION TO S3 SERVICE, SO THAT IT HAS THE PERMISSION TO READ AND WRITE FROM SPECIFID BUCKETS.

## Deletion Operation
* Can replicate delete marker from source to target (Optional setting)
* Deletion with a version ID are not replicatd 
* Replication on worls when verisoning is enabled 

Amazon S3 has a very high durability 
Amazon availability depends on the storge classes 

## Amazon S3 Storage Classes 
1. S3 Standard 
   * Best for Frequently accessed data. 
   * Used when you wnat more than 3 replication or copies
   * Highly avaliable and durability 
   * Easily retrieved 
   * No retrieval fees.

2. S3 Standard IA (Infrequent access) 
   * Best for Infrequently accessed data that still needs rapid access when needed.
   * Lower storage costs than S3 Standard.
   * Retrieval fees apply.
   * High durability and availability.
   * Minimum size on file (128) 
   * Objects shoule be at least 30 days in standard   to move to IA 
   
3. S3 One Zone-Infrequent Access (S3 One Zone-IA)
   * Best for Infrequently accessed data that can be re-created easily if lost.
   * Lower storage costs than S3 Standard-IA.
   * Data is stored in a single Availability Zone, not multiple.
   * Object can be loast if the AZ is destroyed.
   * Less durable and avaliable 
   * lower cost Less critical data 
   * Data should be theer 30 days in stanndard 
  
4. S3 Intelligent Tiering 
* Best for data with unknown or changing access patterns.
* Automatically moves data between frequent and infrequent access tiers based on usage.
* No retrieval fees.

* It automatically moves objects between different access tiers 
    1. Frequent access tier: Designed for objects that are accessed frequently. (Default tier)

    2. Infrequent access â€“ Designed for objects that are accessed less frequently.(Objects not accessed for 30 days)

    3. Archive Instant Access tier: Low cost storage tier optimized for rarely needed objects. (Objects not accessed for 90 days)

    4. Deep Archive Access Tier: Dessigned for objects that are accesss at a maximum of once or twice a year. 

5. S3 Glacier
* Low storage cost for achiving and backup.
* Cost to retrive objects. 
   
S3 Glacier Instant Retrieval
* Best for Long-term archive data that requires immediate retrieval.
* Minimum storage duration is 90days. 
* Milliseconds retrieval time.

S3 Glacier Flexible Retrieval (formerly S3 Glacier)
* Best for long-term archive data with flexible retrieval options.
* Very low storage costs.
* Retrieval times range from minutes to hours.
* Minimum storage duration is 90days. 

S3 Glacieir Deep Archiive 
* Best for long-term archive data with the lowest storage cost.
* Extremely low storage costs.
* Reteivel time takes 12 hours 

## AwWS S3 Lifecylces rules 
* S3 Lifecycle rules allows you to automatically transition objects to different storage classes or delete them after a specified period.

* Lifecycle rules automate the process of moving or deleting objects in your S3 buckets.

* They allow you to define policies that dictate when objects should transition to different storage classes or be deleted.


    1. Transition Actions : Configure object to transition to another storage class.
   
    2. Expiration Actions : Configure object to expire (delete) after some time.

## S3 Requester Pays 
Requester Pays is a feature that shifts the cost of data transfers and requests from the bucket owner to the person or entity making the requests. 

* NORMALLY:
The owner of an S3 bucket is responsible for all costs associated with storing and transferring data.

* Requester Pays enabled: 
The person who downloads or requests data from the S3 bucket pays for those data transfer and request costs. The bucket owner still pays for the storage costs.

Note:
Requesters must have an AWS account to pay for the data transfers.

## S3 Event Notification 
Amazon S3 Event Notifications allow you to trigger actions automatically when certain events occur within your S3 buckets.

* What are the events 
   S3:Objectcreatetd - When an Object is created 
   S3:Objectremopved - When an object is removed

* S3 Event Notifications can integrate with other AWS services, including:
    - Amazon SNS (Simple Notification Service)
    - Amazon SQS (Simple Queue Service)
    - AWS Lambda (serverless computing)

* FOR EVENT NOFICATION TO WORK WE NEED IAM PERMISSIONS
  1. SNS --- Attach SNS Resource (Access) policy 
  2. SQS ---- Attach SQS Resource (Access) policy
  3. Lambda Funstions ---- Attach Lambda Resource policy 

* All the event are sent to the Amazon Event bridge 

## AMAZON EVENT BRIDGE
It helps to connect different AWS applications and services so they can communicate with each other through events.

* Event Bus:
    - EventBridge uses an "event bus" to receive events. This bus acts as a central hub.

* Rules:
    - You create "rules" that define how events should be routed. These rules filter events based on their content. Â  

    - Targets
    When an event matches a rule, EventBridge sends it to a "target," which can be an AWS service like Lambda, SQS, or even a SaaS application.

 ## S3 PERFORMANCE 
 Speeding up object uploads to Amazon S3 involves several strategies that optimize network usage and S3's processing.

 1. Multipart Upload:
    * Instead of uploading a large file as a single piece, you break it into smaller parts and upload them in parallel.
    * This is highly recommended for files larger than 100MB.
    * If a part fails, you only need to re-upload that part, not the entire file. 

2. S3 Transfer Acceleration:
    * Increase the tranfer speed by transfering file to an AWS edge location whcih will forward the data to the S3 bucket in the target region. 

3. S3 Byte-Range Fetches 
    - Paralleize GETs by requesting specific byte ranges. 
    - Used to speed up downloads. 


##  S3 BATCH OPERATIONS 
Allows you to perform large-scale batch operations on S3 objects with a single request.
* Modify object metadata 
* copy object between S3 buckets as a bacth operation. 
* Restore objects 
* Encrypt an ecrypted object 

### In Simple Terms:
Imagine you have a massive filing cabinet (S3 bucket) with millions of files (objects). Instead of manually updating each file, S3 Batch Operations lets you create a "to-do list" (manifest) and tell a robot (S3) to perform the same task (operation) on all the files in the list.

By using S3 Batch Operations, you can efficiently manage your S3 data at scale.


## S3 STORAGE LENS
Amazon S3 Storage Lens is a service that provides organization-wide visibility into your object storage usage and activity. 

Amazon S3 Storage Lens is a service that provides organization-wide visibility into your object storage usage and activity. Think of it as a powerful analytics tool for your S3 data. Here's a beginner-friendly explanation:


####  Why Use S3 Storage Lens?

1. Cost Optimization:
    * Identify expensive storage patterns, such as unused objects or inefficient storage class usage.
    * Find buckets that are good candidates for lifecycle policies.
2. Data Protection:
    * Monitor data replication, encryption, and access control settings.
    * Find buckets without proper encryption.
3. Security:
    * Identify buckets with public access or other security vulnerabilities.
4. Performance:
    * Analyze request patterns and identify performance bottlenecks.
5. Compliance:
    * Track data retention policies and ensure compliance with regulatory requirements.

####  S3 Storage Lens has a default dahboard, where you can visualize summarized insights of s3 objects. 

## S3 Select & Glacier Select 
* S3 select increase query performance and reduced cost 
* It retrieves the specific data nbeeded for the application using SQL by performing server-side filtering 
* It help to reduce network transfer , CPU cost 





# Amazon S3 Server Access Logging
* Server access logging provides detailed records of anything done in an S3 bucket
* S3 server access login is enabled at the bucket level.
* It used for security and access audit purposes.
* When S3 server accces login is enebaled,  every request made to the s3 bucket from any account will be logged into another S3 bucket. 
* This means that we need to create a specific bucket for server access logging 
    Notes: - It better to anable versioning in this new bucket.
           - Also enable MFA DELETE becuae it asks for MFA before anayone can delete anaything in this bucket. 
* Both authorized and unauthorized attempt will be loggen when S3 server access is enabeled.

# CREATE BUCKET IN AWS CLI 
* Create Bucket
 aws s3 mb s3://temi1997/

* Put File in s3 Bucket
$ aws s3 cp Devops/index.html s3://temi1997/

* Bucket Policys
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicRead",
            "Effect": "Allow",
            "Principal": {
                "AWS": "*"
            },
            "Action": [
                "s3:GetObject",
                "s3:GetObjectVersion"
            ],
            "Resource": "arn:aws:s3:::temi1997/*"
        }
    ]
}

## Amazon s3  - Static websiste
* S3 can host a website abd have them sccesible on the internet. 





